---
title: "Predicting the Backing Ratio for the ECCU"
author: "Verne Cazaubon"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r do-not-print-code, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Introduction

The Organisation of Eastern Caribbean States (OECS) is an International Inter-governmental Organisation dedicated to regional integration in the Eastern Caribbean. The OECS is an (11) eleven-member grouping of islands spread across the Eastern Caribbean. Together, they form a near-continuous archipelago across the eastern reaches of the Caribbean Sea. They comprise the Leeward Islands: Antigua and Barbuda, Saint Christopher (St Kitts) and Nevis, Montserrat, Anguilla and the British Virgin Islands; and the Windward Islands: the Commonwealth of Dominica, Saint Lucia, Saint Vincent and the Grenadines and Grenada, Martinique and Guadeloupe. [[link](https://www.oecs.org/en/who-we-are/about-us)].

The Eastern Caribbean Central Bank (ECCB) was established in October 1983. It is the Monetary Authority for a population of more than 600,000 people spanning six sovereign states: Antigua and Barbuda, the Commonwealth of Dominica, Grenada, Saint Christopher (St Kitts) and Nevis, Saint Lucia, and Saint Vincent and the Grenadines, and two overseas territories of the United Kingdom: Anguilla and Montserrat. [[link](https://www.eccb-centralbank.org/overview-eccb)]. These comprise the Eastern Caribbean Currency Union (ECCU). The Eastern Caribbean Dollar (XCD), less formally the EC\$, is pegged to the United States Dollar (USD) at XCD 2.70 = USD 1.00 or EC\$ 2.70 = US\$ 1.00.

In the Governor's Foreword of ECCB's Report and Statement of Accounts for the Financial Year ended 31 March 2023, p.2, he stated that "the Central Bank reports that the EC dollar remains strong and stable with a backing ratio around 92.0 per cent, well above the legal requirement of 60.0 per cent." [[link](https://cdn.eccb-centralbank.org/documents/2023-07-28-01-20-54-Revised-ECCB-2022-2023-Annual-Report-and-Financial-Statements-27-July-2023.pdf)]. What is this backing ratio? And why is it important?

First, the backing ratio is the ratio of Total External Assets (International Reserves) to Total Demand Liabilities. In other words, $\text{Backing Ratio} = \frac{\text{Total External Assets}}{\text{Total Demand Liabilities}}$.

Now to understand its importance, consider the following from the same document. Under the section titled Foreign Reserves Management, p. 62, it was stated that "The Bank will seek to strengthen the management of Foreign Reserves by: 1. Relooking the risk appetite of the Bank while keeping in mind the backing ratio". Here we note that the backing ratio is of extreme importance to foreign reserves management, particularly given the exchange rate agreement of the currency union, that is, being pegged to the USD. Thus, critical decisions taken by the ECCB always consider the impact on the backing ratio.

With the above in mind, we set out to predict the backing ratio of the ECCU in an effort to provide a tool to monitor the indicator, project its trajectory, and eventually assist with decision making at the highest level of the organisation.

\newpage

## Abbreviations and Acronyms

**Organisations:**

* ECCB - Eastern Caribbean Central Bank
* ECCU - Eastern Caribbean Currency Union
* OECD - Organisation for Economic Co-operation and Development
* OECS - Organisation of Eastern Caribbean States

**Countries:**

* CAN - Canada
* CHN - People's Republic of China
* GBR - United Kingdom
* JPN - Japan
* USA - United States of America

**Other:**

* CPI - Consumer Price Index
* csv - comma-separated values
* GFC - Global Financial Crisis

## Description of data sets

For the purpose of this analysis, an extremely limited data set will be considered. It will consider data obtained from the websites of the ECCB  [[link](https://www.eccb-centralbank.org/)] and the OECD [[link](https://www.oecd.org/)]. From the ECCB's website, the total external assets and total demand liabilities were downloaded in csv format. The indicators downloaded from OECD's website were forecasts for: CPI or inflation, long-term interest rates, and unemployment rates.

Forecast data was used as opposed to the actual data because when predicting the backing ratio only forecast data would be available at that moment. We note that this would increase error but it presents a more realistic scenario. For the target variable (backing ratio), however, the actual values were selected as our goal is to predict it.

In the end, thirteen (13) independent variables (will also be referred to as predictors, predictor variables, or features) were used to forecast target variable. It is expected that in the full implementation of the project more than thirteen independent variables will be selected. More on this will be mentioned in the section *Future Work*.

## Goal of project

The goal of this project is to predict the backing ratio of the ECCU given forecast economic data from the main international trading partners and  tourism source markets of the ECCU countries. These main international trading partners and tourism source markets are: Canada (CAN), People's Republic of China (CHN), Japan (JPN), United Kingdom (GBR), and United States of America (USA). It should be noted that two indicators, long-term interest rates forecasts and unemployment rates forecasts, were not available for the People's Republic of China from the OECD's database. In a full scale implementation of this project, the goal is to produce results at least as accurate as those of any econometric model that is used to predict the backing ratio.

## Key steps performed

An outline of the key programming steps performed are as follows:

1. Install packages if necessary, and load the required libraries.
2. Download data from the respective websites/databases.
3. Input the data.
4. Clean the data.
5. Select the relevant records and independent variables (or features).
6. Visualize the data. Gain insight into variables/features.
7. Combine the data into one tibble. Format the data to ready it for analysis.
8. Separate the data into a training set and a test set.
9. Analyse the data using different machine learning algorithms.
10. Compare the performance of these algorithms.
11. Conclude on which model best fits the data.

```{r install-and-load-libraries, message = FALSE}
# automatically install missing packages
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(readxl)) install.packages("readxl", repos = "http://cran.us.r-project.org")
if(!require(ggcorrplot)) install.packages("ggcorrplot", repos = "http://cran.us.r-project.org")
if(!require(zoo)) install.packages("zoo", repos = "http://cran.us.r-project.org")
if(!require(kernlab)) install.packages("kernlab", repos = "http://cran.us.r-project.org")
if(!require(pls)) install.packages("pls", repos = "http://cran.us.r-project.org")
if(!require(xgboost)) install.packages("xgboost", repos = "http://cran.us.r-project.org")

# load necessary libraries
library(tidyverse)
library(caret)
library(knitr)
library(readxl)
library(ggcorrplot) 
library(zoo)
library(kernlab)
library(pls)
library(xgboost)
```

\newpage

# Analysis

## Processes and techniques used in data cleaning

Data were obtained from different websites and as a result the downloaded data were formatted differently. Therefore the process used to clean data sets was dependent on the website the data came from. The format of downloaded data from any one website is similar which makes cleaning data from that website an easier process when multiple files need to be inputted and cleaned.

For this project, one csv file was downloaded from the ECCB's Statistics webpage and stored in a folder named Data. The file was read into the program. It had no header row. There was some metadata in the first few lines of the file followed by column headings in row 8. After inputting the data the column headings were renamed to those in the file and the extra rows at the beginning were deleted. Next, the column the column types were inspected to know how to proceed. The data were cleaned so as to make analysis possible. This included converting dates from character to date format, and converting the Total External Assets and Total Demand Liabilities from character to numeric. We also checked for any records containing NAs so those would have been handled prior to moving on, if any were found. Finally, the Backing Ratio was calculated by dividing the item in column Total External Assets by the item in column Total Demand Liabilities for each record. Intermediate tibbles used in the cleaning process were deleted to preserve computer memory.

Three csv files were downloaded from the OECD's Data webpage and stored in a folder named Data. As the files came from the same data source, the process to clean the data were similar for each file. The files were read into the program. Their  columns were examined to see what kind of data they contained. The files already had column headings but the names of some of those headings needed to renamed to something more readable. NAs were checked for and dealt with if any were found. Dates were converted to a date format. This conversion was more computationally involved than for the previous data set as the dates in these tables were formatted as year-quarter, for example 1980-Q2. This format could not be handled by any packages that were previously loaded. Once this was done, the countries and dates that were needed were filtered out into a tibble, and intermediate tibbles were deleted.

```{r import-and-clean-data, include = FALSE}
### Import data to, and calculate the backing ratio for the ECCU.

raw_backing_ratio <- read.csv("Data/Backing_ratio_Q_ECCU.csv")

# rename the columns, then delete the opening rows of the file.
colnames(raw_backing_ratio) <- raw_backing_ratio[7,]
raw_backing_ratio <- raw_backing_ratio[-(1:7),]

# inspect column data types.
str(raw_backing_ratio)

# widen data to easily calculate backing ratio.
wide_backing_ratio <- pivot_wider(raw_backing_ratio, names_from = `Indicator Label`, values_from = Amount)

# rename column heading to easily reference columns.
wide_backing_ratio <- rename(wide_backing_ratio, Total_External_Assets = `Total External Assets (International Reserves)`, Total_Demand_Liabilities = `Total Demand Liabilities`)

# check for NAs.
sum(is.na(wide_backing_ratio$Total_External_Assets))
sum(is.na(wide_backing_ratio$Total_Demand_Liabilities))

# convert dates from chr to date format.
wide_backing_ratio$Date <- ymd(wide_backing_ratio$Date)

# convert columns from chr to num. Commas must be removed from chr first to avoid NAs, therefore, parse_number function utilized.
wide_backing_ratio$Total_External_Assets <- parse_number(wide_backing_ratio$Total_External_Assets)
wide_backing_ratio$Total_Demand_Liabilities <- parse_number(wide_backing_ratio$Total_Demand_Liabilities)
wide_backing_ratio$Total_External_Assets <- as.numeric(as.character(wide_backing_ratio$Total_External_Assets))
wide_backing_ratio$Total_Demand_Liabilities <- as.numeric(as.character(wide_backing_ratio$Total_Demand_Liabilities))

# convert data frame to tibble.
as_tibble(wide_backing_ratio)

# calculate backing ratio and append to table, then visualize ratio on a line graph.
backing_ratio_eccu <- wide_backing_ratio %>% select(Date, Unit, Total_External_Assets, Total_Demand_Liabilities) %>% arrange(Date) %>% mutate(backing_ratio = Total_External_Assets/Total_Demand_Liabilities*100)

### Calculation of backing ratio completed.

### Import inflation forecast data for OECD countries.

raw_inflation_OECD <- read.csv("Data/Inflation_forecasts_Q_OECD.csv")

# inspect column data types.
str(raw_inflation_OECD)

# widen data for easier interpretation.
wide_inflation_OECD <- pivot_wider(raw_inflation_OECD, names_from = INDICATOR, values_from = Value)

# rename column heading to easily reference columns.
wide_inflation_OECD <- rename(wide_inflation_OECD, Country = ï..LOCATION, Date = TIME, CPI_forecast = CPIFORECAST)

# check for NAs.
sum(is.na(wide_inflation_OECD$CPI_forecast))

# convert TIME from Q1 ... Q4 values to date format.
year <- str_extract(wide_inflation_OECD$Date, "(19|20)\\d{2}")
qdate <- str_extract(wide_inflation_OECD$Date, "Q\\d+")
qdate <- str_replace(qdate, "Q1", "03-31")
qdate <- str_replace(qdate, "Q2", "06-30")
qdate <- str_replace(qdate, "Q3", "09-30")
qdate <- str_replace(qdate, "Q4", "12-31")
wide_inflation_OECD$Date <- paste(year, qdate, sep = '-')
wide_inflation_OECD$Date <- ymd(wide_inflation_OECD$Date)

# convert data frame to tibble.
as_tibble(wide_inflation_OECD)

### End of "inflation forecast data for OECD countries" wrangling.

### Import long term interest rates forecast data for OECD countries.

raw_interest_rates_OECD <- read.csv("Data/Long_term_interest_rates_forecast_Q_OECD.csv")

# inspect column data types.
str(raw_interest_rates_OECD)

# widen data for easier interpretation.
wide_interest_rates_OECD <- pivot_wider(raw_interest_rates_OECD, names_from = INDICATOR, values_from = Value)

# rename column heading to easily reference columns.
wide_interest_rates_OECD <- rename(wide_interest_rates_OECD, Country = ï..LOCATION, Date = TIME, Interest_forecast = LTINTFORECAST)

# check for NAs.
sum(is.na(wide_interest_rates_OECD$Interest_forecast))

# convert TIME from Q1 ... Q4 values to date format.
year <- str_extract(wide_interest_rates_OECD$Date, "(19|20)\\d{2}")
qdate <- str_extract(wide_interest_rates_OECD$Date, "Q\\d+")
qdate <- str_replace(qdate, "Q1", "03-31")
qdate <- str_replace(qdate, "Q2", "06-30")
qdate <- str_replace(qdate, "Q3", "09-30")
qdate <- str_replace(qdate, "Q4", "12-31")
wide_interest_rates_OECD$Date <- paste(year, qdate, sep = '-')
wide_interest_rates_OECD$Date <- ymd(wide_interest_rates_OECD$Date)

# convert data frame to tibble.
as_tibble(wide_interest_rates_OECD)

### End of "long term interest rates forecast data for OECD countries" wrangling.

### Import unemployment rates forecast data for OECD countries.

raw_unemployment_OECD <- read.csv("Data/Unemployment_rates_forecast_Q_OECD.csv")

# inspect column data types.
str(raw_unemployment_OECD)

# widen data for easier interpretation.
wide_unemployment_OECD <- pivot_wider(raw_unemployment_OECD, names_from = INDICATOR, values_from = Value)

# rename column heading to easily reference columns.
wide_unemployment_OECD <- rename(wide_unemployment_OECD, Country = ï..LOCATION, Date = TIME, Unemployment_forecast = UNEMPFORECAST)

# check for NAs.
sum(is.na(wide_unemployment_OECD$Unemployment_forecast))

# convert TIME from Q1 ... Q4 values to date format.
year <- str_extract(wide_unemployment_OECD$Date, "(19|20)\\d{2}")
qdate <- str_extract(wide_unemployment_OECD$Date, "Q\\d+")
qdate <- str_replace(qdate, "Q1", "03-31")
qdate <- str_replace(qdate, "Q2", "06-30")
qdate <- str_replace(qdate, "Q3", "09-30")
qdate <- str_replace(qdate, "Q4", "12-31")
wide_unemployment_OECD$Date <- paste(year, qdate, sep = '-')
wide_unemployment_OECD$Date <- ymd(wide_unemployment_OECD$Date)

# convert data frame to tibble.
as_tibble(wide_unemployment_OECD)

### End of "unemployment rates forecast data for OECD countries" wrangling.
```

## Processes and techniques used in visualization and data exploration

Due to the time series nature of the data, line graphs were primarily used for visualization. We now examine the variables/features to see whether they coincide with events that we know. Sometimes the data may reveal events which we were not aware of, and would prompt an investigation into the cause. Any extreme values (outliers) seen were worth investigating.

## Insights gained

We now move to see what insight we can gain from the data. What story is the data telling? How does the story of the independent variables, or features, coincide or differ from that of the target variable? We look at visualizations of each of these in turn, then we consider a correlation matrix showing any relationships among the variables.

\newpage

### Backing ratio

By examining the graph below it can be seen that the backing ratio has remained above 87.5% through the entire period. Two events occurred which noticeably affected the backing ratio for an extended spell. The first occurred around 2008-2009 which coincides with the Global Financial Crisis (GFC). To this point the backing ratio has never returned to the pre-GFC level. The second event commenced sometime in 2020 and lasted approximately two years. This, of course, would be the COVID-19 pandemic. The backing ratio seems to just be recovering from this last economic shock. 

```{r visualize-br}
#visualize the backing ratio on a line graph
backing_ratio_eccu %>% ggplot(aes(Date, backing_ratio)) + geom_line() + scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) + geom_hline(yintercept = 60) + annotate("text", x = as_date("2000-01-01"), y = 63, label = "Legal minimum = 60%", hjust = 0) + xlab("Year") + ylab("Backing Ratio")
```

\newpage

### Inflation

Bear in mind that the graph below, and those which follow, show forecast data and not actual data. Also noteworthy is that they contain forecasts through to Q4 2024.

The CPI forecast shows similar trends to the backing ratio. All countries show huge spikes in the CPI around the same economic shocks of the GFC and the pandemic. Over the first two decades there are other spikes seen by individual or multiple countries, however, it is only during the two major economic shocks that these spikes coincide for all five countries.

It is also evident that through most of the series, Japan's inflation remained relatively low and comparably stable. In addition to the spikes in inflation around the GFC and pandemic, Japan's data reveals that another significant economic shock occurred sometime in 2011. That would be when a powerful earthquake struck the northeastern part of Japan causing a huge tsunami which killed thousands and devastated that part of the country.

```{r visualize-inflation}
# select countries which are major source markets for tourism and partners for trade
select_countries <-  c('CAN', 'CHN', 'GBR', 'JPN', 'USA')

# visualize select countries' inflation over period
inflation_OECD <- wide_inflation_OECD %>% select(Country, Date, CPI_forecast)
inflation_OECD %>% filter(Country %in% select_countries) %>% filter(Date >= "2000-03-31") %>% group_by(Country) %>% ggplot(aes(Date, CPI_forecast, col = Country)) + geom_line() + geom_hline(yintercept = 0)  + xlab("Year") + ylab("Inflation forecast")
```

\newpage

### Interest rates

The interest rates forecasts of Canada, the United Kingdom, and the United States of America all seem to move very closely with each other. Japan's interest rate forecasts are always below that of the other three countries, however, it still follows their trend. The graph below shows that interest rate forecasts have trended downward from 2000 to 2020. From the end of 2020, though, we are seeing the sharpest increase in interest rates ever. In just two years, interest rates seem to reach levels seen a decade prior. The forecasts through the end of 2023 and through 2024 do show a leveling off and slight decrease for Canada, the United Kingdom, and the United States of America. Japan's interest rates are forecast to continue rising.

```{r visualize-interest-rates}
# visualize select countries' interest rates over period
interest_rates_OECD <- wide_interest_rates_OECD %>% select(Country, Date, Interest_forecast)
interest_rates_OECD %>% filter(Country %in% select_countries) %>% filter(Date >= "2000-03-31") %>% group_by(Country) %>% ggplot(aes(Date, Interest_forecast, col = Country)) + geom_line() + geom_hline(yintercept = 0)  + xlab("Year") + ylab("Interest rate forecast")
# note that the People's Republic of China is absent from the data
```

\newpage

### Unemployment

Unemployment, similar to the previous two features/variables, saw an increase around the GFC and pandemic. At the start of 2020, unemployment was at its 20-year low for all countries shown. At the start of the pandemic, unemployment rates forecasts rose sharply for all countries. The forecasts for Canada and the United States of America spiked to levels unseen in the first two decades of the century. These forecasts quickly fell but are on the rise through 2024. Contrarily, Japan's and the United Kingdom's forecasts for unemployment are heading downward through 2024.

```{r visualize-unemployment}
# visualize select countries' unemployment over period
unemployment_OECD <- wide_unemployment_OECD %>% select(Country, Date, Unemployment_forecast)
unemployment_OECD %>% filter(Country %in% select_countries) %>% filter(Date >= "2000-03-31") %>% group_by(Country) %>% ggplot(aes(Date, Unemployment_forecast, col = Country)) + geom_line() + geom_hline(yintercept = 0)  + xlab("Year") + ylab("Unemployment forecast")
# note that the People's Republic of China is absent from the data
```

```{r delete-unnecessary-tibbles}
# remove data frames that are no longer necessary.
rm(raw_backing_ratio, wide_backing_ratio)

# remove data frames that are no longer necessary.
rm(raw_inflation_OECD, wide_inflation_OECD)

# remove data frames that are no longer necessary.
rm(raw_interest_rates_OECD, wide_interest_rates_OECD)

# remove data frames that are no longer necessary.
rm(raw_unemployment_OECD, wide_unemployment_OECD)
```

```{r join-into-one-tibble}
### Combine data into one tibble.

# select all the target variable data, and dates.
br <- backing_ratio_eccu %>% select(Date, backing_ratio)

# select only necessary data from the inflation_OECD tibble. Filter countries. Select dates within the same range as that of the target variable.
infOECD <- inflation_OECD %>% filter(Country %in% select_countries) %>% filter(Date >= "2000-03-31") %>% select(Country, Date, CPI_forecast)

# widen the data so that each country's data gets its own column.
infOECD <- pivot_wider(infOECD, names_from = Country, values_from = CPI_forecast)

# rename all the columns with suffix "CPI".
colnames(infOECD) <- paste(colnames(infOECD),"CPI",sep="_")

# revert to the name "Date" for the date column.
infOECD <- rename(infOECD, Date = 'Date_CPI')

# select only necessary data from the interest_rates_OECD tibble. Filter countries. Select dates within the same range as that of the target variable.
intOECD <- interest_rates_OECD %>% filter(Country %in% select_countries) %>% filter(Date >= "2000-03-31") %>% select(Country, Date, Interest_forecast)

# widen the data so that each country's data gets its own column.
intOECD <- pivot_wider(intOECD, names_from = Country, values_from = Interest_forecast)

# rename all the columns with suffix "interest".
colnames(intOECD) <- paste(colnames(intOECD),"interest",sep="_")

# revert to the name "Date" for the date column.
intOECD <- rename(intOECD, Date = 'Date_interest')

# select only necessary data from the unemployment_OECD tibble. Filter countries. Select dates within the same range as that of the target variable.
unempOECD <- unemployment_OECD %>% filter(Country %in% select_countries) %>% filter(Date >= "2000-03-31") %>% select(Country, Date, Unemployment_forecast)

# widen the data so that each country's data gets its own column.
unempOECD <- pivot_wider(unempOECD, names_from = Country, values_from = Unemployment_forecast)

# rename all the columns with suffix "unemployment".
colnames(unempOECD) <- paste(colnames(unempOECD),"unemployment",sep="_")

# revert to the name "Date" for the date column.
unempOECD <- rename(unempOECD, Date = 'Date_unemployment')

# combine into one tibble, joining records by date.
econ_data <- left_join(br, infOECD, by = join_by(Date))
econ_data <- left_join(econ_data, intOECD, by = join_by(Date))
econ_data <- left_join(econ_data, unempOECD, by = join_by(Date))

# remove tibbles that are no longer necessary.
rm(br, infOECD, intOECD, unempOECD)

# For this implementation of the project, remove the date column as it will not be used from this point forward.
econ_data <- subset(econ_data, select = -Date)

### Data are now ready for analysis.
```

\newpage

After visualizing each data set individually, the data were combined into one tibble. We now look at the relationships between the different variables/features. The correlation matrix (or correlogram) below was used to examine the correlations/relationships between the features/independent variables and the target variable. It showed that there was a strong positive correlation between the interest rate forecasts of Canada, Japan, the United Kingdom, and the United States of America. This is not unusual as it was seen in the line graph earlier. There were also positive correlations among the countries with regard to unemployment and CPI forecasts, although the relationship were not as strong as that of interest rates.

Also noteworthy from the correlation matrix is the moderate negative correlation between unemployment and inflation. Again, this relationship is not unusual.

The backing ratio showed a moderate positive correlation with the People's Republic of China's inflation. Its relationship with other features was weak to moderately negative. There was very weak to no correlation with Japan's inflation, Canada's unemployment, and the United Kingdom's unemployment.

```{r near-zero-variance, include = FALSE}
# Pre-processing

# test for near zero variance so that certain variable/features can be removed.
# nearZeroVar returns the index of columns which have near zero variance.
nzv <- nearZeroVar(econ_data)
nzv
# no variable/feature showed near zero variance. Therefore, keep all, or find another way of deleting some.
```

```{r corr-matrix}
# create and visualize correlation matrix to gain insight into data.
correlation_matrix <- round(cor(econ_data),2)
ggcorrplot(correlation_matrix, method = 'square')

# Very weak to no association: 0.0 to 0.2
# Weak correlation: 0.2 to 0.4
# Moderate correlation: 0.4 to 0.6
# strong correlation: 0.6 to 0.8
# Very strong correlation: 0.8 to 1.0

# rename target variable to "y" for easier referencing.
econ_data <- rename(econ_data, y = 'backing_ratio')
```

## Modeling approach

The data from this new single tibble was used for further analysis. The data were divided into two new data subsets - a training set and a test set. Regression algorithms were used as opposed to classification as our target variable, the backing ratio, is a continuous variable. The algorithms were supervised learning algorithms and were fed the training set to learn on. After learning, their performance was evaluated using the test set.

The algorithms utilized were: generalized linear model (baseline), k-nearest neighbors, random forest, support vector machine with a polynomial kernel, principal component analysis, and extreme gradient boosting. We also created an ensemble model which dropped the highest and lowest prediction of the five non-baseline algorithms, and computed the mean of the remaining three as its prediction.

The root mean square error (RMSE) was used as the loss function to analyse the performance of the models. The formula is
$\text{RMSE}(\hat{y}, y) = \sqrt{\frac{\sum_{i=1}^{N} (\hat{y}_i - y_i)^2}{N}}$.
The units of RMSE would be the same units as the predicted variable - backing ratio, which is percent (%). Therefore, a RMSE < 1 would be a good target particularly since we are using forecast data and not actual data as predictors. This would be interpreted as, on average, the model predicted values within +1% or -1% from the actual value.

The required indicators from the ECCB's website were available only back to the year 2000. The data were available with quarterly periodicity up to the second quarter (Q2) of 2023. As this data forms the target variable, the independent variables had to be limited to this same time frame (Q1 2000 to Q2 2023). This meant that only 94 records of data were available for the process. Due to the small number of records, 90% of the data were used to train the algorithms and 10% to test them.

To tune the algorithms' parameters, the methods used included 10-fold cross-validation, 5-fold cross-validation, and the default bootstrap method.

```{r rmse}
# Root Mean Square Error function to judge model performance.
RMSE <- function(predicted_value, observed_value){
  sqrt(mean((predicted_value - observed_value)^2))
}
```

```{r partition-data}
# Partition data into training and testing sets.
# Due to size of data set, 90% of the data will be used for training and 10% for testing.
# Please note that in this initial implementation of the project, data will be treated as if independent and identically distributed (i.i.d.).
# For future implementations, the data are time series data and will be treated as such.

seed <- 1983 # set seed for reproducibility. As an ECCB nugget, 1983 will be used.
set.seed(seed) # if using version later than R 3.5, add sample.kind = "Rounding" argument to set.seed.

test_index <- createDataPartition(y = econ_data$y, times = 1,
                                  p = 0.1, list = FALSE)
train_set <- econ_data[-test_index,]
test_set <- econ_data[test_index,]
```

## Generalized Linear Model (Baseline model)

As a baseline, a generalized linear model was used. This is at its core linear regression, however, it can be used to fit non-linear data - and the backing ratio is non-linear. There are no parameters to be tuned for this mode.

```{r glm, warning = FALSE}
# Use of machine learning models for regression.
# build the model with backing_ratio as target and all other variables/features as predictors.

### Generalized Linear Model (glm) will be used as the baseline model.

set.seed(seed)

# glm can fit both linear and non-linear data.
train_glm <- train(y ~ ., method = "glm", data = train_set)

# create a data frame to record the performance indicators on the training set.
train_results <- data_frame(getTrainPerf(train_glm))

# make predictions on the target variable using the test_set.
y_hat_glm <- predict(train_glm, test_set, type = "raw")

# convert output to a numeric format.
y_hat_glm <- as.numeric(y_hat_glm)

#calculate RMSE of glm prediction versus test data.
glm_value <- RMSE(y_hat_glm, test_set$y)
```

The results to this point are:

```{r glm-result}
# create data frame to record RMSEs of the algorithms on the test set.
rmse_results <- data_frame(Method= "Generalized Linear Model (Baseline)", RMSE_value = glm_value)

# print the data frame to the console.
rmse_results %>% knitr::kable()

### End of glm (baseline model)
```

\newpage

## k-Nearest Neighbors

This algorithm outputs the value that is the mean of the k-nearest neighbors of the input value as a prediction. 10-fold validation used to tune k, the number of neighbors. As seen below, the result of cross-validation was choosing k = 3. Therefore, the predicted value of an inputted data point is, in the simplest sense, the mean of its three closest neighbors. In practice, of course, it is more complicated.

```{r knn}
### k-Nearest Neighbors

set.seed(seed)

# use 10-fold cross validation to tune knn algorithm instead of default bootstrap method.
control_10fold <- trainControl(method = "cv", number = 10, p = .9)
train_knn_cv <- train(y ~ ., method = "knn",
                      data = train_set,
                      tuneGrid = data.frame(k = seq(1, 11, 1)),
                      trControl = control_10fold)

# visualize results of cross validation.
ggplot(train_knn_cv, highlight = TRUE)  + xlab("Number of neighbors, k") + ylab("RMSE (10-fold cross-validation)")

# details of model selected.
#train_knn_cv$finalModel

# append training performance indicators of the training set to the train_results data frame.
train_results <- bind_rows(train_results, data_frame(getTrainPerf(train_knn_cv)))

# make predictions on the target variable using the test_set.
y_hat_knn <- predict(train_knn_cv, test_set, type = "raw", k = train_knn_cv$bestTune)

# calculate RMSE of knn prediction versus test data.
knn_value <- RMSE(y_hat_knn, test_set$y)

```

After running the algorithm using the optimal parameter, the results to this point are:

```{r knn-result}

# print result to data frame.
rmse_results <- bind_rows(rmse_results,
                          data_frame(Method="k-Nearest Neighbours",
                                     RMSE_value = knn_value))

# print the data frame to the console.
rmse_results %>% knitr::kable()

### End of knn
```

\newpage

## Random Forest

This algorithm is an ensemble algorithm that outputs the mean prediction of the individual decision trees that are formed during the training process. The default bootstrap method used to tune the mtry parameter. The method selected mtry = 3. That is, using 3 randomly selected predictors returned the smallest RMSE.

```{r rf, eval = TRUE}
### Random Forest

set.seed(seed)

# default bootstrap method utilized.
train_rf <- train(y ~ ., method = "rf", data = train_set, tuneGrid = data.frame(mtry = seq(1, 13, 1)))

# visualize results of bootstrap method.
ggplot(train_rf, highlight = TRUE)  + xlab("Number of randomly select predictors, mtry")

# details of model selected.
#train_rf$finalModel

# append training performance indicators of the training set to the train_results data frame.
train_results <- bind_rows(train_results, data_frame(getTrainPerf(train_rf)))

# make predictions on the target variable using the test_set.
y_hat_rf <- predict(train_rf, test_set, type = "raw", mtry = train_rf$bestTune)

# convert output to a numeric format.
y_hat_rf <- as.numeric(y_hat_rf)

# calculate RMSE of rf prediction versus test data.
rf_value <- RMSE(y_hat_rf, test_set$y)
```

After running the algorithm using the optimal parameter, the results to this point are:

```{r rf-result}
# print result to data frame.
rmse_results <- bind_rows(rmse_results,
                          data_frame(Method="Random Forest",
                                     RMSE_value = rf_value))

# print the data frame to the console.
rmse_results %>% knitr::kable()

### End of rf
```

\newpage

## Support Vector Machine with Polynomial Kernel

This algorithm uses hyper-planes (essentially multidimensional lines or planes) to make decisions. The hyper-plane which yields the greatest distance between input points is used in the optimal model. A polynomial kernel was used as the predicted variable is not expected to follow a linear model. The default bootstrap method used to tune the polynomial degree, scale, and C parameters of the algorithm. That method resulted in degree = 3, scale = 0.09, and C = 0.1 as the optimal parameters. This is seen in the diagram below.

```{r svmPoly, eval = TRUE}
### Support Vector Machine with Polynomial Kernel

set.seed(seed)

# default bootstrap method utilized.
train_svmPoly <- train(y ~ ., method = "svmPoly", data = train_set, tuneGrid = expand.grid(degree = seq(1, 5, 1), scale = seq(0.01, 0.15, 0.01), C = seq(0.1, 0.5, 0.1)))

# visualize results of bootstrap method.
ggplot(train_svmPoly, highlight = TRUE)

# details of model selected.
#train_svmPoly$finalModel

# append training performance indicators of the training set to the train_results data frame.
train_results <- bind_rows(train_results, data_frame(getTrainPerf(train_svmPoly)))

# make predictions on the target variable using the test_set.
y_hat_svmPoly <- predict(train_svmPoly, test_set, type = "raw", degree = train_svmPoly$bestTune$degree, scale = train_svmPoly$bestTune$scale, C = train_svmPoly$bestTune$C)

# calculate RMSE of svmPoly prediction versus test data.
svmPoly_value <- RMSE(y_hat_svmPoly, test_set$y)
```

After running the algorithm using the optimal parameters, the results to this point are:

```{r symPoly-result}
# print result to data frame.
rmse_results <- bind_rows(rmse_results,
                          data_frame(Method="Support Vector Machine with Polynomial Kernel",
                                     RMSE_value = svmPoly_value))

# print the data frame to the console.
rmse_results %>% knitr::kable()

### End of svmPoly
```

\newpage

## Principal Component Analysis

This algorithm attempts to reduce the number of predictor variables (or components) by selecting those which explain the most variance in the target variable and are independent among each other. These reduced number of predictor variables are then used to predict the output. 5-fold cross validation was used to tune the parameter, ncomp, number of components. It determined that 12 components were optimal as seen in the diagram below.

```{r pcr}
### Principal Component Analysis

set.seed(seed)

# use 5-fold cross-validation.
control_5fold <- trainControl(method = "cv", number = 5, p = .9)
train_pcr <- train(y ~ ., method = "pcr", data = train_set, tuneGrid = data.frame(ncomp = seq(1, 14, 1)), trControl = control_5fold)

# visualize results of cross validation.
ggplot(train_pcr, highlight = TRUE)  + xlab("Number of components, ncomp") + ylab("RMSE (5-fold cross-validation)")

# details of model selected.
#train_pcr$finalModel

# append training performance indicators of the training set to the train_results data frame.
train_results <- bind_rows(train_results, data_frame(getTrainPerf(train_pcr)))

# make predictions on the target variable using the test_set.
y_hat_pcr <- predict(train_pcr, test_set, type = "raw", ncomp = train_pcr$bestTune)

# calculate RMSE of pcr prediction versus test data.
pcr_value <- RMSE(y_hat_pcr, test_set$y)
```

After running the algorithm using the optimal parameter, the results to this point are:

```{r pcr-result}
# print result to data frame.
rmse_results <- bind_rows(rmse_results,
                          data_frame(Method="Principal Component Analysis",
                                     RMSE_value = pcr_value))

# print the data frame to the console.
rmse_results %>% knitr::kable()

### End of pcr
```

\newpage

## eXtreme Gradient Boosting

This algorithm is similar to a random forest in that decision trees are a key element in the decision making process. They differ in that random forest create predictions in parallel, whereas this algorithm links the decision trees sequentially. The algorithm was run with default parameters which would have been self tuned. There are four (4) parameters which would have had to be tuned otherwise, but we will not get into these in this project.

```{r xgboost, eval = TRUE}
### eXtreme Gradient Boosting

set.seed(seed)

# default parameters allowed.
train_xgboost <- train(y ~ ., method = "xgbLinear", data = train_set)

# append training performance indicators of the training set to the train_results data frame.
train_results <- bind_rows(train_results, data_frame(getTrainPerf(train_xgboost)))

# make predictions on the target variable using the test_set.
y_hat_xgboost <- predict(train_xgboost, test_set, type = "raw")

# calculate RMSE of xgboost prediction versus test data.
xgboost_value <- RMSE(y_hat_xgboost, test_set$y)
```

After running the algorithm the results to this point are:

```{r xgboost-result}
# print result to data frame.
rmse_results <- bind_rows(rmse_results,
                          data_frame(Method="eXtreme Gradient Boosting",
                                     RMSE_value = xgboost_value))

# print the data frame to the console.
rmse_results %>% knitr::kable()

### End of xgboost
```

## Ensemble

Our very own ensemble model was also created using the 20% trimmed mean. The highest and lowest predictions of the five non-baseline algorithms were cut off and the mean of the other three algorithms' predictions was returned as its prediction.

```{r ensemble, eval=TRUE, include = FALSE}
### Ensemble

# Combine results of all algorithms except the baseline.
alg_results <- data_frame(knn = y_hat_knn, pcr = y_hat_pcr, rf = y_hat_rf, svmPoly = y_hat_svmPoly, xgboost = y_hat_xgboost)

# use the mean after the highest and lowest values are removed.
ensemble_pred <- alg_results %>% rowwise() %>% mutate(trimmed_means = mean(c_across(c(knn, pcr, rf, svmPoly, xgboost)), trim=0.2)) %>% select(trimmed_means)
as_tibble(ensemble_pred)

# calculate RMSE of ensemble prediction versus test data.
ensemble_value <- RMSE(ensemble_pred$trimmed_means, test_set$y)
```

After running the algorithm the results to this point are:

```{r ensemble-result}
# print result to data frame.
rmse_results <- bind_rows(rmse_results,
                          data_frame(Method="Ensemble",
                                     RMSE_value = ensemble_value))

# print the data frame to the console.
rmse_results %>% knitr::kable()

### End of ensemble
```

\newpage

# Results

## Modeling results

As seen in the tables below all RMSE results on the test set were better than the RMSE results on the training set. All models improved the performance on the test set. Also observe that all of the RMSEs were better than that of the baseline model, Generalized Linear Model, which is a generalized linear regression. k-nearest neighbors outperformed all other models by far. It was the only algorithm to reach the target of having a RMSE < 1.

```{r rmse-results, eval=TRUE}
#print train_results
train_results %>% knitr::kable()

# sort data frame by RMSE values so that the algorithm with the least value is first.
rmse_results <- rmse_results %>% arrange(RMSE_value)

# print the data frame to the console.
rmse_results %>% knitr::kable()
```

## Discussion of model performance

When looking at another element of performance, that is time to execute the algorithm, it was observed that Random Forest's bootstrap method was lengthy but it's run on the test set was efficient. The Support Vector Machine with Polynomial Kernel and the eXtreme Gradient Boosting algorithms took the longest to run with the latter taking up the most time. However, these did not result in the best RMSE.

It should also be noted that if the seed is changed, the above results and the performance of the algorithms may change. If this is the case, then the best algorithm may be the Ensemble model rather than k-nearest neighbors.

\newpage

# Conclusion

## Brief summary of report

The report shows that all algorithms are able to predict the backing ratio with root mean square errors ranging from `r knn_value` to `r glm_value` including the ensemble model which had a root mean square error of `r ensemble_value`.

The best performing algorithm and only algorithm to reach the performance target was k-nearest neighbors with the RMSE of `r knn_value` which may be because of the time series nature of the data, or perhaps randomly because of the seed that was chosen. The next best algorithm was the Ensemble with a RMSE of `r ensemble_value`.

## Potenital Impact

The project will continue to be refined as if it is able to predict the backing ratio with an acceptable level of accuracy, then it can be used to aid in policy creation. In addition to predicting the trajectory of the backing ratio, it could be used to determine the effect of risk appetite on backing ratio. This could eventually become a great asset to the ECCB.

## Limitations

There are several limitations to this project. The first is that only a small number of records are available on the ECCB's website. More data are available in-house, that is in the database of the ECCB, and so that would increase the number of records available for analysis.

Another limitation is the small number of predictor variables considered here to limit size of project. Additionally, they were utilized without any transformations being done. Also, only variables which had forecast data were used. This would likely lead to a decrease in the ability of the algorithms to predict accurately, and therefore an increase in RMSE. The RMSE would likely be reduced if actual values of the predictor variables were used. However, when predicting something that occurs in the future, only forecast data would be available. If the backing ratio had to be "now-cast", then the actual values of the independent variables could have been considered.

The training and test sets were chosen randomly. However, this method would not be ideal for time series data as the information that the previous point would provide is lost. For future  implementations the sampling process would have to be reconsidered.

## Future work

In a full scale implementation of the project, data from the International Monetary Funds' (IMF) World Economic Outlook (WEO) database, the Federal Reserve Bank of St Louis' Federal Reserve Economic Data (FRED) database, Google Finance, and Yahoo Finance would be considered.

These websites contain indicators of interest which include but are not limited to: benchmark rates, bond and stock funds, credit spreads, equity prices, gross domestic product (better known as GDP, for ECCU countries as well as international countries of interest), tourism indicators, volatility indices, and yields. Some of these indicators won't have forecasts and so forecasts would have to be made for them before being used in the updated model.

The available periodicity (how frequently the data are compiled) of the above indicators vary widely (from daily to annually) and so thought has to be put into how the NAs would be dealt with for those available with lower periodicity such as semi-annually and annually.

Consider using method = timeslice with appropriate options for initialWindow, horizon, and fixedWindow in trainControl when cross validating. That is, treat the data as time series data instead of independent, identically distributed data.

With an increase in the number of predictor variables, consideration would have to be given to variables showing higher correlations to the target variable. Although algorithms such as Principal Component Analysis would take that into consideration, other algorithms may need assistance.

Due to the time series nature of the data, another variable to consider would be inclusion of a lag variable, specifically the lag of the backing ratio. As both components of the backing ratio (reserve assets and demand liabilities) are stock variables, as opposed to flow variables, the closing position of the backing ratio in one period is related to its closing position at the end of the subsequent period. As it stands in this project, the target variable was predicted independent of this lag. However, being time series data, there should be some link to the lag of the target variable. The number of periods of lag would depend on the periodicity of the target variable.

The predictor variables need to be examined for stationarity, that is whether the mean (trend), variance and autocorrelation are constant. If these do not hold, then transformations of the data would improve performance. These transformations could include but are not limited to differencing the data and performing log transformations on variables. Analysis of the residuals of the models to determine whether correlations exist can also be performed. If there are correlations it could imply that the model is not considering a key variable.

The above, as well as other considerations would definitely build on this project and provide a stepping stone to improve performance with the next implementation.

# References

Sources for data sets
Other resources used.
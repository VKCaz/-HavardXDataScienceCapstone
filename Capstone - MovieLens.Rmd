---
title: "MovieLens Capstone"
author: "Verne Cazaubon"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Introduction

In October 2006, Netflix launched a challenge to create a recommendation system that beat Cinematch (Netflix's in-house recommendation system) by at least ten (10) percent. That is, the new system had to better predict the movies that customers would like. The grand prize was one million United States Dollars!

So what is a recommendation system? And are there any practical examples of its uses?

Recommendation systems use ratings that users give an item on buying and/or using them to make specific recommendations. Companies like Amazon collect massive data sets of user ratings on the products sold to them. The data sets are subsequently used to predict high rated items for a given user and then recommended them to the user. 

Similarly, Netflix uses movie ratings provided by users to predict a high rated movie for a given user and then recommends it to the user. Usually the movie ratings are on a 1-5 scale, where 5 represents an excellent movie and 1 suggests it to be a poor one. For this project, the ratings are from 0.5 to 5.

## Description of dataset

The MovieLens data set included in the course material is a subset of a larger data set. The stripped down version contains approximately ten (10) million data records. Each record is comprised of six (6) variables: userId, movieId, rating, timestamp, title, and genres.

## Summary of goal of project

The aim of this project is to create a movie recommendation system using concepts taught throughout the previous eight (8) courses of edX's HarvardX Data Science Professional Certificate program. This recommendation system utilizes a supervised machine learning algorithm to predict user ratings of movies given certain features.

## Key steps performed

An outline of the key programming steps performed are as follows:

1. Install packages if necessary, and load the required libraries.
2. Download the files.
3. Combine the data into one file.
4. Visualize the data. Gain insight into variables. 
5. Separate the data into a training set and a test set.
6. Analyse the data using different machine learning algorithms.
7. Evaluate the performance of these algorithms, and choose an algorithm as the final model.
8. Run the final_holdout_test on the chosen model.

```{r install-and-load-libraries, message = FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(forcats)) install.packages("forcats", repos = "http://cran.us.r-project.org")
if(!require(recosystem)) install.packages("recosystem", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(dplyr)
library(stringr)
library(knitr)
library(lubridate)
library(forcats)
```

# Analysis

## Processes and techniques used in data cleaning

The data were downloaded from the grouplens website. Two files were downloaded, one containing the ratings data and another containing the movie data. The "movieId" variable linked the two files. The two files were joined into one, based on this link variable. Then, the  data were partitioned into an edx data set and a final_holdout_test data set. All unnecessary data frames were then deleted to preserve memory. There was very little data cleaning to be done. 

```{r download-data, include=FALSE}
# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

options(timeout = 120)

dl <- "ml-10M100K.zip"
if(!file.exists(dl))
  download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings_file <- "ml-10M100K/ratings.dat"
if(!file.exists(ratings_file))
  unzip(dl, ratings_file)

movies_file <- "ml-10M100K/movies.dat"
if(!file.exists(movies_file))
  unzip(dl, movies_file)

ratings <- as.data.frame(str_split(read_lines(ratings_file), fixed("::"), simplify = TRUE),
                         stringsAsFactors = FALSE)
colnames(ratings) <- c("userId", "movieId", "rating", "timestamp")
ratings <- ratings %>%
  mutate(userId = as.integer(userId),
         movieId = as.integer(movieId),
         rating = as.numeric(rating),
         timestamp = as.integer(timestamp))

movies <- as.data.frame(str_split(read_lines(movies_file), fixed("::"), simplify = TRUE),
                        stringsAsFactors = FALSE)
colnames(movies) <- c("movieId", "title", "genres")
movies <- movies %>%
  mutate(movieId = as.integer(movieId))

movielens <- left_join(ratings, movies, by = "movieId")

# Final hold-out test set will be 10% of MovieLens data

seed <- 1
# set.seed(seed, sample.kind="Rounding") # if using R 3.6 or later
set.seed(seed) # if using R 3.5 or earlier
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in final hold-out test set are also in edx set
final_holdout_test <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from final hold-out test set back into edx set
removed <- anti_join(temp, final_holdout_test)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```

## Processes and techniques used in data exploration and visualization

```{r explore-data, include=FALSE}
#####Some of the code which follows was used to respond to the exam questions on edx.
#Exploratory data analysis

str(edx) %>% knitr::kable()

num_rec <- nrow(edx)
```

By examining the structure and head of the edx data set we note that it contains the six variables “userID”, “movieID”, “rating”, “timestamp”, “title”, and “genres”. Each row represent a single rating of a user for a single movie. The first six (6) records can be seen in the table below. There are no missing values (NAs) in the data set. It contains `r num_rec` records.

```{r explore-data-head, tidy=TRUE}
head(edx) %>% knitr::kable()
```

```{r check-for-NAs, include=FALSE}
findNAs <- is.na(edx$rating)
sum(findNAs)

findNAs <- is.na(edx$title)
sum(findNAs)

findNAs <- is.na(edx$userId)
sum(findNAs)

findNAs <- is.na(edx$movieId)
sum(findNAs)

findNAs <- is.na(edx$genres)
sum(findNAs)

findNAs <- is.na(edx$timestamp)
sum(findNAs)

```

### Movies

The top 10 titles in the data set are as follows:

```{r top10titles, include=TRUE}
edx %>%
  group_by(title) %>%
  summarize(count = n()) %>%
  arrange(-count) %>%
  top_n(10, count) %>%
  ggplot(aes(count, reorder(title, count))) +
  geom_bar(color = "black", fill = "grey44", stat = "identity") +
  geom_text(aes(label=count), hjust=-0.1, size = 2) +
  xlab("Number of ratings") + ylab(NULL) + xlim(0, 40000)

#Question 3
num_movies <- n_distinct(edx$movieId)
```

There are `r num_movies` distinct movies in the data set.

### Users

The distribution of user ratings is as follows:

```{r expl-usr, include=TRUE}

edx %>% group_by(userId) %>%
  summarize(count = n()) %>%
  ggplot(aes(count)) +
  geom_histogram(color = "black", fill = "grey44", bins = 40) +
  xlab("Number of Ratings") +
  ylab("Number of Users") +
  scale_x_log10()

#Question 4
num_users <- n_distinct(edx$userId)
```

There are `r num_users` distinct users in the data set.

### Genres

The genres sorted by count are as follows:

```{r expl-genre, include=TRUE}

#top genres table
top_genr <- edx %>% separate_rows(genres, sep = "\\|") %>%
  group_by(genres) %>%
  summarize(count = n()) %>%
  arrange(desc(count))

top_genr %>% knitr::kable()

```

### Ratings

A 6-number summary of the ratings is:
```{r expl-rating, include=TRUE}

edx %>% select(rating) %>% summary() %>% knitr::kable()
```

```{r ans-edx-questions, include=FALSE}
#Question 2
#sum(edx$rating == 0)
sum(edx$rating == 0.5)
sum(edx$rating == 3)

# Does the following code reproduce the results of Q3 and Q4? Yes, it does.
# movielens %>%
#   summarize(n_movies = n_distinct(movieId), n_users = n_distinct(userId))

#Question 5
genre_ratings <- edx %>% group_by(genres) %>% summarise(count = n())

index <- str_detect(genre_ratings$genres, "([A-Za-z]*\\|)*Drama(\\|[A-Za-z]*)*")
sum(index * genre_ratings$count)

index <- str_detect(genre_ratings$genres, "([A-Za-z]*\\|)*Comedy(\\|[A-Za-z]*)*")
sum(index * genre_ratings$count)

index <- str_detect(genre_ratings$genres, "([A-Za-z]*\\|)*Thriller(\\|[A-Za-z]*)*")
sum(index * genre_ratings$count)

index <- str_detect(genre_ratings$genres, "([A-Za-z]*\\|)*Romance(\\|[A-Za-z]*)*")
sum(index * genre_ratings$count)

#Question 6
most_ratings <- edx %>% group_by(title) %>% summarise(count = n()) %>% arrange(desc(count))

#Question 7
freq_ratings <- edx %>% group_by(rating) %>% summarise(count = n()) %>% arrange(desc(count))
```

The distribution of ratings is as follows:

```{r distr-ratings}
group <-  ifelse((edx$rating == 1 |edx$rating == 2 | edx$rating == 3 | 
                  edx$rating == 4 | edx$rating == 5) ,
                   "whole_star", 
                   "half_star") 

explore_ratings <- data.frame(edx$rating, group)

ggplot(explore_ratings, aes(x= edx.rating, fill = group)) +
  geom_histogram( binwidth = 0.2) +
  scale_x_continuous(breaks=seq(0, 5, by= 0.5)) +
  scale_fill_manual(values = c("half_star"="grey44", "whole_star"="grey22")) +
  labs(x="Rating", y="Number of ratings")
#####
```

## Insights gained

From the above visualizations it was observed that the most popular movies are older movies. The top 10 movies were made in years ranging from 1977 to 1995.

The distribution of number of user ratings is right-skewed. The majority of users have less than 100 ratings.

The most popular genres are drama, comedy, and action, whereas the least favorite are film-noir, documentary, and IMAX.

There are more whole star ratings than there are half star ratings. The most common rating is 4 stars.

```{r train-and-test-sets, include=FALSE}
# Create an additional partition of training and test sets from the provided

# set.seed(seed, sample.kind="Rounding") # if using R 3.6 or later
set.seed(seed) # if using R 3.5 or earlier

test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)
train_set <- edx[-test_index,]
temp <- edx[test_index,]

# To make sure we don't include users and movies in the test set that do not appear in the training set, we removed these.
test_set <- temp %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

# Add rows removed from test_set set back into train_set
removed <- anti_join(temp, test_set)
train_set <- rbind(train_set, removed)

rm(test_index, temp, removed)
```

## Modeling approach

RMSE will be used as the loss function to compare the performance of the machine learning algorithms employed. The formula is $\text{RMSE} = \sqrt{\frac{\sum_{u,i} (\hat{y}_{u,i}-y_{u,i})^{2}}{N}}$ where $\hat{y}_{u,i}$ represents the prediction of the rating of movie $i$ by user $u$. $y_{u,i}$ is the observed rating of movie $i$ by user $u$. And $N$ is the total number of user-movie combinations. The RMSE can be interpreted similar to standard deviation. If the RMSE = 1, as an example, this means that the average error between the predicted value is the observed value is one star. This project aims to get a RMSE < 0.86490.

```{r rmse}
# Loss function - Root Mean Squared Error (RMSE) can be interpreted as standard deviation.
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

The machine learning techniques utilized  are regression models, and a recommender package that uses matrix factorization to address the problem. We now consider these more in depth.

### Naive model

```{r naive-model, include=FALSE}
### Building the Recommendation System

# A first model
mu_hat <- mean(train_set$rating)
mu_hat
```

This very basic model predicts that every user will give every movie the average/mean rating. For our training set this mean is equal to `r mu_hat`. It assumes that any deviation from this value is explained by random variation. This model is represented by the equation: 

$Y_{u, i} = \mu + \epsilon_{u, i}$

where $\mu$ is the true rating for all movies and users, and $\epsilon_{u, i}$ represents the independent errors per user per movie sampled from the same distribution that is centered at zero. The result of this model is

```{r naive-model-contd}

naive_rmse <- RMSE(test_set$rating, mu_hat)
#naive_rmse

# The following code confirms that any number other than mu_hat would result into a higher RMSE
#predictions <- rep(2.5, nrow(test_set))
#RMSE(test_set$rating, predictions)

# append results to tibble.
rmse_results <- tibble(Model = "Naive - mean user rating", RMSE = naive_rmse)
rmse_results %>% knitr::kable()

#End of naive model
```

\newpage

### Movie effect

This model adds a new term, $b_{i}$,  representing the average rating for movie $i$. This model is represented by the equation  

$Y_{u, i} = \mu + b_{i} + \epsilon_{u, i}$

Note that $b_{i}$ is the average of $Y_{u, i} - \mu$ for each movie $i$.

We see below that the $\hat{b}_{i}$ form a left skewed normal distribution ranging from -3 to 1.5 with most values around zero. This is not unusual as $\hat{\mu}$ was `r mu_hat` and we saw earlier that the ratings are distributed between 0.5 and 5.

```{r movie-effect, warning=FALSE}
# Modeling movie effect

mu <- mean(train_set$rating)
movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))

movie_avgs %>% qplot(b_i, geom ="histogram", bins = 30, data = ., color = I("black")) + xlab("Movie bias") + ylab("Count")

predicted_ratings_movie_effect <- mu + test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  pull(b_i)
```

The results of models to this point are

```{r movie-effect-result, warning=FALSE}
model_1_rmse <- RMSE(predicted_ratings_movie_effect, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          tibble(Model="Movie Effect",
                                     RMSE = model_1_rmse ))
rmse_results %>% knitr::kable()

# End of Movie Effect Model
```

\newpage

### Movie + user effect

This model adds a new term, $b_{u}$  representing the average rating for user $u$. This model is represented by the equation  

$Y_{u, i} = \mu + b_{i} + b_{u} + \epsilon_{u, i}$

Note that $b_{u}$ is the average of $Y_{u, i} - \mu - b_{i}$ for each user $u$. We will only include users who have rated at least 100 movies. This distribution appears slightly skewed also, although not as much as with $\hat{b}_{i}$. As expected, the distribution ranges from 0.5 to 5.

```{r movie-user-effect}
# Modeling Movie + User Effect

train_set %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating)) %>% 
  filter(n()>=100) %>%
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = "black") + xlab("User bias") + ylab("Count")

user_avgs <- train_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

predicted_ratings_movie_user_effect <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)
```

The results of models to this point are

```{r movie-user-effect-result}
model_2_rmse <- RMSE(predicted_ratings_movie_user_effect, test_set$rating)
#model_2_rmse

rmse_results <- bind_rows(rmse_results,
                          tibble(Model="Movie + User Effect",  
                                     RMSE = model_2_rmse ))
rmse_results %>% knitr::kable()

# End of Movie + User Effect Model
```

\newpage

### Regularized movie effect

Regularization penalizes extreme estimates (whether large or small) that are formed using small sample sizes, as is the case when a small number of users rate a movie. In such cases, the prediction is shrunk to a more conservative value in an effort to decrease errors and thereby reduce RMSE.

Using regularization to estimate the movie effect, we estimate the $b$'s by minimizing the equation 
$\frac{1}{N} \sum_{u,i} ( y_{u,i} - \mu - b_i)^2 + \lambda \sum_{i} b_{i}^2$. 

The optimal penalty $\lambda$ to use can be found using cross-validation. It will minimize the equation 

$\hat{b}_{i} (\lambda) = \frac{1}{\lambda + n_{i}} \sum_{u=1}^{n_{i}} (Y_{u,i} - \hat{\mu})$ 

where $n_i$ is the number of ratings for movie $i$.

```{r reg-movie, include=FALSE}

## Regularization

#Regularized Movie Effect Model

lambdas <- seq(0, 10, 0.25)
model_3_rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set$rating)
  just_the_sum <- train_set %>%
    group_by(movieId) %>% 
    summarize(s = sum(rating - mu), n_i = n())
  predicted_ratings_reg_movie_effect <- test_set %>% 
    left_join(just_the_sum, by='movieId') %>% 
    mutate(b_i = s/(n_i+l)) %>%
    mutate(pred = mu + b_i) %>%
    pull(pred)
  return(RMSE(predicted_ratings_reg_movie_effect, test_set$rating))
})
```

We apply cross validation to obtain the optimal penalty, $\lambda$. The plot below shows the value of $\lambda$ producing the lowest RMSE.

```{r reg-movie-lambdas}
qplot(lambdas, model_3_rmses)  + xlab("Lambda") + ylab("RMSE")
lambda <- lambdas[which.min(model_3_rmses)]
#lambda
```

The optimal value of $\lambda$ was found to be `r lambda`. The results of models to this point are

```{r reg-movie-results}
rmse_results <- bind_rows(rmse_results,
                          tibble(Model="Regularized Movie Effect",  
                                     RMSE = min(model_3_rmses) ))
rmse_results %>% knitr::kable()

#End of Regularized Movie Effect Model
```

\newpage

### Regularized movie + user effect

Using regularization to estimate the movie and user effect, we estimate the $b$'s by minimizing the equation 

$\frac{1}{N} \sum_{u,i} ( y_{u,i} - \mu - b_i - b_u)^2 + \lambda (\sum_{i} b_{i}^2 + \sum_{u} b_{u}^2)$. 

The optimal penalty $\lambda$ to use can be found using cross-validation. It will minimize the equations

$\hat{b}_{i} (\lambda) = \frac{1}{\lambda + n_{i}} \sum_{u=1}^{n_{i}} (Y_{u,i} - \hat{\mu})$ and 

$\hat{b}_{u} (\lambda) = \frac{1}{\lambda + n_{i}} \sum_{u=1}^{n_{i}} (Y_{u,i} - \hat{\mu} - \hat{b}_i)$.

```{r reg-movie-user}
#Regularized Movie + User Effect Model

lambdas <- seq(0, 10, 0.25)
model_4_rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set$rating)
  b_i <- train_set %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  b_u <- train_set %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  predicted_ratings_reg_movie_user_effect <- 
    test_set %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  return(RMSE(predicted_ratings_reg_movie_user_effect, test_set$rating))
})
```

We apply cross validation to obtain the optimal penalty, $\lambda$. The plot below shows the value of $\lambda$ producing the lowest RMSE.

```{r reg-movie-user-lambdas}
qplot(lambdas, model_4_rmses) + xlab("Lambda") + ylab("RMSE")
lambda_rmu <- lambdas[which.min(model_4_rmses)]
#lambda_rmu
```

The optimal value of $\lambda$ was found to be `r lambda_rmu`. The results of models to this point are

```{r reg-movie-user-results}
rmse_results <- bind_rows(rmse_results,
                          tibble(Model="Regularized Movie + User Effect",  
                              RMSE = min(model_4_rmses)))
rmse_results %>% knitr::kable()    

#End of Regularized Movie + User Effect Model
```

### Matrix Factorization

For this method data are processed as a large and sparse matrix, then decomposed into two smaller dimensional matrices with latent features and less sparsity. To make the process more efficient the recosystem package will be used. We start by converting data into the recosystem format, find the best tuning parameters, train and finally test it.

```{r matrix-factorization, include=FALSE, eval=TRUE}

#Parallel Matrix Factorization using recosystem package

library(recosystem)

# set.seed(seed, sample.kind="Rounding") # if using R 3.6 or later
set.seed(seed) # if using R 3.5 or earlier

#put data into acceptable format for package
train <- with(train_set, data_memory(user_index = userId, item_index = movieId, rating = rating))
test <- with(test_set, data_memory(user_index = userId, item_index = movieId, rating = rating))

#create a reco object
r <- Reco()

#tune the recommender system's parameters. This is time intensive.
tuned_parameters <- r$tune(train, opts = list(dim = c(20, 30),
                                            costp_l2 = c(0.01, 0.1),
                                            costq_l2 = c(0.01, 0.1),
                                            lrate = c(0.01, 0.1),
                                            nthread = 4,
                                            niter = 10))

#train using tuned parameters
r$train(train, opts = c(tuned_parameters$min, nthread = 4, niter = 30))

#make a prediction
predicted_ratings_matrix_factorization <- r$predict(test, out_memory())

#compare prediction to actual values
model_5_rmse <- RMSE(predicted_ratings_matrix_factorization, test_set$rating)
```

The results of models to this point are

```{r matrix-factorization-result, eval=TRUE}
#report RMSE
rmse_results <- bind_rows(rmse_results,
                          tibble(Model="Matrix Factorization",  
                                     RMSE = model_5_rmse))
rmse_results %>% knitr::kable() 

#End of Parallel Matrix Factorization Model

```

# Results

## Modeling results

The results of all models are as follows:

```{r final_holdout_test, include=FALSE, eval=TRUE}
# Below, use best model on final_holdout_test

# set.seed(seed, sample.kind="Rounding") # if using R 3.6 or later
set.seed(seed) # if using R 3.5 or earlier

edx_train <- with(edx, data_memory(user_index = userId, item_index = movieId, rating = rating))
holdout_test <- with(final_holdout_test, data_memory(user_index = userId, item_index = movieId, rating = rating))
r <- Reco()

#parameters have already been tuned.

r$train(edx_train, opts = c(tuned_parameters$min, nthread = 4, niter = 30))

predicted_ratings_matrix_factorization <- r$predict(holdout_test, out_memory())

model_5_rmse_holdout <- RMSE(predicted_ratings_matrix_factorization, final_holdout_test$rating)
```

```{r final_holdout_test-result, eval=TRUE}
rmse_results <- bind_rows(rmse_results,
                          tibble(Model="Matrix Factorization (on final_holdout_test)",  
                                 RMSE = model_5_rmse_holdout))
rmse_results %>% arrange(RMSE) %>% knitr::kable() 

#End of Parallel Matrix Factorization Model on final_holdout_test

###

```

## Discussion of model performance

The winner is clear among this group of models. Matrix factorization produces a RMSE far below the target of 0.86490. It's RMSE is `r model_5_rmse`.

All models improved on the baseline model's performance. The regularized models also beat out their corresponding counterparts proving that regularization is effective. It can be seen that the regularized move + user effect model performed just below the 0.86490 target. However, this may not have been the case if the same model ran on the final_holdout_test data set.

# Conclusion

## Brief summary of report

This project examined numerous algorithms to predict the ratings of movies by users of a subset of MovieLens data.  The algorithms' performances were evaluated using root mean squared error (RMSE). A linear regression model utilizing regularization (regularized movie + user effect) provided very good performance but could not outdo the matrix factorization method. Although they both performed below the target of 0.86490, the matrix factorization method also beat the target with the final_holdout_test data set. It's RMSE on the final_holdout_set was `r model_5_rmse_holdout`.

## Potential impact

Performance gains on any recommendation system would result in algorithms better able to predict what we like based on our past ratings. This would reduce the need to choose from an overwhelming number of goods and services recommendations that we are bombarded with on a daily basis, particularly e-commerce and media related.

## Limitations

The best performing algorithm in this project was very computationally demanding having to manipulate structures containing tens of millions of items. That algorithm takes approximately 50 minutes to run on an old i7-5500U system with 15.7 GB of usable RAM. If we can get similar performance from an algorithm that takes $\frac{1}{10}^{th}$ of the time, we would go with the latter, even at the expense of performance.

## Future work

In the future we will consider other effects such as movie + user + genre effect and movie + user + time effect and if possible regularized versions of these. We will also consider the use of principal component analysis, support vector machines, eXtreme gradient boosting, and other recommender systems. An ensemble model will also be created.